{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy \n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "from pathlib import Path\n",
    "import torch_scatter\n",
    "\n",
    "from typing import *\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.insert(0, str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.logging\n",
    "utils.logging.setup(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narde rules\n",
    "https://www.bkgm.com/variants/Narde.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    class Step(Enum):\n",
    "        IDLE = \"idle\"\n",
    "        ROLL = \"roll\"\n",
    "        TURN = \"trun\"\n",
    "        FINISHED = \"finished\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_seed(seed: int | bool | None) -> int:\n",
    "        if isinstance(seed, bool):\n",
    "            seed = random.randint(0, 2**32-1) if seed else None\n",
    "\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        return seed\n",
    "\n",
    "    def __init__(self, seed: bool | int | None = None, verbose = False):\n",
    "        self.seed = self._init_seed(seed)\n",
    "        self.board = np.zeros(24, dtype=int)\n",
    "        self.board[0] = 15\n",
    "        self.board[12] = -15\n",
    "        self.dice = [0, 0]\n",
    "        self.home = [0, 0]\n",
    "        self.pturn = 0\n",
    "        self.t = 0\n",
    "        self.head_moves = 0\n",
    "        self.valid_moves = None\n",
    "        self.step : Game.Step = Game.Step.IDLE\n",
    "        self.loglevel = logging.INFO if verbose else logging.DEBUG\n",
    "        logger.log(self.loglevel, f\"new game started, seed={self.seed}\")\n",
    "\n",
    "    def _get_next_pos(self, pos: int, steps: int) -> int:\n",
    "        dst_pos = pos + steps\n",
    "        if dst_pos > 24:\n",
    "            dst_pos = dst_pos % 25 + 1\n",
    "        if dst_pos < 1:\n",
    "            dst_pos = 24 + dst_pos\n",
    "        return dst_pos\n",
    "    \n",
    "    def _get_head(self, player: Optional[int] = None) -> int:\n",
    "        player = player if player is not None else self.pturn\n",
    "        return 1 if player == 0 else 13\n",
    "    \n",
    "    def _get_user_sign(self, player: Optional[int] = None) -> int:\n",
    "        player = player if player is not None else self.pturn\n",
    "        return 1 if player == 0 else -1\n",
    "\n",
    "    def _can_move_home(self):\n",
    "        counter = self.home[self.pturn]\n",
    "        home = range(19, 25) if self.pturn == 0 else range(7, 13)\n",
    "        for pos in home:\n",
    "            if self._has_checkers(pos, player=self._cur_player):\n",
    "                counter += self._get_checkers(pos, player=self._cur_player)\n",
    "        return counter == 15\n",
    "\n",
    "    def _is_move_home(self, pos: int, steps: int) -> bool:\n",
    "        dst_pos = self._get_next_pos(pos, steps)\n",
    "        return (self.pturn == 0 and dst_pos < pos) or \\\n",
    "               (self.pturn == 1 and pos <= 12 and dst_pos > 12)\n",
    "    \n",
    "    @property\n",
    "    def _opponent(self) -> int:\n",
    "        return 1 if self.pturn == 0 else 0\n",
    "    \n",
    "    @property\n",
    "    def _cur_player(self) -> int:\n",
    "        return self.pturn\n",
    "    \n",
    "    def _has_checkers(self, pos: int, player: Optional[int] = None):\n",
    "        return self._get_checkers(pos, player) > 0\n",
    "    \n",
    "    def _get_checkers(self, pos: int, player: Optional[int] = None):\n",
    "        player = player if player is not None else self.pturn\n",
    "        return self._get_user_sign(player) * self.board[pos-1]\n",
    "    \n",
    "    def _find_prime(self, pos: int) -> Optional[Tuple[int, int]]:\n",
    "        seq = 1\n",
    "        result = [pos, pos]\n",
    "        next_ptr = self._get_next_pos(pos, 1)\n",
    "        while seq < 6 and self._has_checkers(next_ptr, player=self._cur_player):\n",
    "            seq += 1\n",
    "            result[1] = next_ptr\n",
    "            next_ptr = self._get_next_pos(next_ptr, 1)\n",
    "        prev_ptr = self._get_next_pos(pos, -1)\n",
    "        while seq < 6 and self._has_checkers(prev_ptr, player=self._cur_player):\n",
    "            seq += 1\n",
    "            result[0] = prev_ptr\n",
    "            prev_ptr = self._get_next_pos(prev_ptr, -1)\n",
    "        return tuple(result) if seq == 6 else None\n",
    "    \n",
    "    def _is_blocking_prime(self, dst_pos: int) -> bool:\n",
    "        prime_range = self._find_prime(dst_pos)\n",
    "        if prime_range:\n",
    "            prime_end_pos = prime_range[1]\n",
    "            for step in range(1, 24):\n",
    "                search_pos = self._get_next_pos(prime_end_pos, step)\n",
    "                if self._get_head(self._opponent) == search_pos:\n",
    "                    return True # we reached other player's home without finding it's checkers\n",
    "                if self._has_checkers(pos=search_pos, player=self._opponent):\n",
    "                    break\n",
    "        return False\n",
    "    \n",
    "    def _check_move(self, pos: int, steps: int):\n",
    "        dst_pos = self._get_next_pos(pos, steps)\n",
    "        if self.step != Game.Step.TURN:\n",
    "            raise RuntimeError(\"invalid action\")\n",
    "        if not (1 <= pos <= 24):\n",
    "            raise RuntimeError(\"invalid position\")\n",
    "        if not self._has_checkers(pos, player=self._cur_player):\n",
    "            raise RuntimeError(f\"no checkers at position {pos}\")\n",
    "        if self._has_checkers(dst_pos, player=self._opponent):\n",
    "            raise RuntimeError(f\"can't move to position {dst_pos}\")\n",
    "        if steps not in self.dice:\n",
    "            raise RuntimeError(f\"no dice with value {steps}\")\n",
    "        if self._is_move_home(pos, steps):\n",
    "            if not self._can_move_home():\n",
    "                raise RuntimeError(f\"not all checkers are at finishing table\")\n",
    "        if self._get_head() == pos and self.head_moves > 0:\n",
    "            if not (self.head_moves == 1 and self.dice[0] == self.dice[1] and self.t < 2):\n",
    "                raise RuntimeError(f\"can't make any more head moves\")\n",
    "        if self._is_blocking_prime(dst_pos):\n",
    "            raise RuntimeError(f\"can't form a blocking prime\")\n",
    "        # TODO: If player can play one number but not both, they must play the higher one\n",
    "\n",
    "    def _render_player(self, player: Optional[int] = None, lower: bool = True) -> str:\n",
    "        player = player if player is not None else self.pturn\n",
    "        result = \"O\" if player == 0 else \"X\"\n",
    "        return result.lower() if lower else result\n",
    "\n",
    "    def _is_valid_move(self, pos: int, steps: int) -> bool:\n",
    "        try:\n",
    "            self._check_move(pos, steps)\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            return False\n",
    "    \n",
    "    def _enum_valid_moves(self) -> Iterator[Tuple[int, int]]:\n",
    "        for pos in range(1, 25):\n",
    "            if self._has_checkers(pos, player=self._cur_player):\n",
    "                for steps in range(1, 7):\n",
    "                    if self._is_valid_move(pos, steps):\n",
    "                        yield (pos, steps)\n",
    "\n",
    "    def get_valid_moves(self) -> List[Tuple[int, int]]:\n",
    "        if self.valid_moves is None:\n",
    "            self.valid_moves = list(self._enum_valid_moves())\n",
    "        return self.valid_moves\n",
    "    \n",
    "    def has_valid_moves(self):\n",
    "        return len(self.get_valid_moves()) > 0\n",
    "\n",
    "    def start(self, d1: int = 0, d2: int = 0) -> \"Game\":\n",
    "        if self.step != Game.Step.IDLE:\n",
    "            raise RuntimeError(\"invalid action\")\n",
    "\n",
    "        self.dice = [d1 or random.randint(1, 6), d2 or random.randint(1, 6)]\n",
    "        while self.dice[0] == self.dice[1]:\n",
    "            self.dice = [random.randint(1, 6), random.randint(1, 6)]\n",
    "\n",
    "        self.step = Game.Step.ROLL\n",
    "        if self.dice[0] > self.dice[1]:\n",
    "            self.pturn = 0\n",
    "        else: # self.dice[0] < self.dice[1]:\n",
    "            self.pturn = 1\n",
    "        return self\n",
    "\n",
    "    def roll(self, d1: int = 0, d2: int = 0) -> \"Game\":\n",
    "        if self.step != Game.Step.ROLL:\n",
    "            raise RuntimeError(\"invalid action\")\n",
    "        self.dice = [d1 or random.randint(1, 6), d2 or random.randint(1, 6)]\n",
    "        logger.log(self.loglevel, f\"t={self.t}, p={self._render_player()} rolls {self.dice}\")\n",
    "        if self.dice[0] == self.dice[1]:\n",
    "            self.dice += self.dice\n",
    "        self.step = Game.Step.TURN\n",
    "        return self\n",
    "\n",
    "    def turn(self, pos: int, steps: int) -> \"Game\":\n",
    "        dst_pos = self._get_next_pos(pos, steps)\n",
    "        self._check_move(pos, steps)\n",
    "        if self._is_move_home(pos, steps):\n",
    "            logger.log(self.loglevel, f\"t={self.t}, p={self._render_player()} moves: {pos}->HOME\")\n",
    "            self.board[pos-1] -= self._get_user_sign()\n",
    "            self.home[self.pturn] += 1\n",
    "        else:\n",
    "            logger.log(self.loglevel, f\"t={self.t}, p={self._render_player()} moves: {pos}-({steps})->{dst_pos}\")\n",
    "            self.board[pos-1] -= self._get_user_sign()\n",
    "            self.board[dst_pos-1] += self._get_user_sign()\n",
    "\n",
    "        if self._get_head() == pos:\n",
    "            self.head_moves += 1\n",
    "\n",
    "        if len(self.dice) > 2:\n",
    "            self.dice.pop(self.dice.index(steps, -1))\n",
    "        else:\n",
    "            self.dice[self.dice.index(steps)] = 0\n",
    "\n",
    "        if self.home[self.pturn] == 15:\n",
    "            logger.log(self.loglevel, f\"t={self.t}, game finished, p={self._render_player()} wins\")\n",
    "            self.step = Game.Step.FINISHED\n",
    "        elif self.dice[0] == 0 and self.dice[1] == 0:\n",
    "            self.step = Game.Step.ROLL\n",
    "            self.pturn = self._opponent\n",
    "            self.t += 1\n",
    "            self.head_moves = 0\n",
    "\n",
    "        self.valid_moves = None\n",
    "        return self\n",
    "    \n",
    "    def is_finished(self):\n",
    "        return self.step == Game.Step.FINISHED\n",
    "    \n",
    "    def skip(self) -> \"Game\":\n",
    "        if self.step != Game.Step.TURN:\n",
    "            raise RuntimeError(\"invalid action\")\n",
    "        \n",
    "        if self.has_valid_moves():\n",
    "            raise RuntimeError(\"skip only possible when there's no moves\")\n",
    "        else:\n",
    "            logger.log(self.loglevel, f\"t={self.t}, p={self._render_player()} has no eligible moves, skipping\")\n",
    "            self.dice = [0, 0]\n",
    "            self.step = Game.Step.ROLL\n",
    "            self.pturn = (self.pturn + 1) % 2\n",
    "            self.t += 1\n",
    "            self.head_moves = 0\n",
    "            \n",
    "        self.valid_moves = None\n",
    "        return self\n",
    "\n",
    "    def __repr__(self):\n",
    "        template = \"\"\"\n",
    "        |{oha}| 24 | 23 | 22 | 21 | 20 | 19 |{xst}| 18 | 17 | 16 | 15 | 14 | 13 |{xho}|\n",
    "        |{ohb}|-----------------------------|     |-----------------------------|{xhn}|\n",
    "        |{ohc}|{x1}|{w1}|{v1}|{u1}|{t1}|{s1}|     |{r1}|{q1}|{p1}|{o1}|{n1}|{m1}|{xhm}|\n",
    "        |{ohd}|{x2}|{w2}|{v2}|{u2}|{t2}|{s2}|     |{r2}|{q2}|{p2}|{o2}|{n2}|{m2}|{xhl}|\n",
    "        |{ohe}|{x3}|{w3}|{v3}|{u3}|{t3}|{s3}|     |{r3}|{q3}|{p3}|{o3}|{n3}|{m3}|{xhk}|\n",
    "        |{ohf}|{x4}|{w4}|{v4}|{u4}|{t4}|{s4}|     |{r4}|{q4}|{p4}|{o4}|{n4}|{m4}|{xhj}|\n",
    "        |{ohg}|{x5}|{w5}|{v5}|{u5}|{t5}|{s5}|     |{r5}|{q5}|{p5}|{o5}|{n5}|{m5}|{xhi}|\n",
    "        |{ohh}|-----------------------------|{dcs}|-----------------------------|{xhh}|\n",
    "        |{ohi}|{a5}|{b5}|{c5}|{d5}|{e5}|{f5}|     |{g5}|{h5}|{i5}|{j5}|{k5}|{l5}|{xhg}|\n",
    "        |{ohj}|{a4}|{b4}|{c4}|{d4}|{e4}|{f4}|     |{g4}|{h4}|{i4}|{j4}|{k4}|{l4}|{xhf}|\n",
    "        |{ohk}|{a3}|{b3}|{c3}|{d3}|{e3}|{f3}|     |{g3}|{h3}|{i3}|{j3}|{k3}|{l3}|{xhe}|\n",
    "        |{ohl}|{a2}|{b2}|{c2}|{d2}|{e2}|{f2}|     |{g2}|{h2}|{i2}|{j2}|{k2}|{l2}|{xhd}|\n",
    "        |{ohm}|{a1}|{b1}|{c1}|{d1}|{e1}|{f1}|     |{g1}|{h1}|{i1}|{j1}|{k1}|{l1}|{xhc}|\n",
    "        |{ohn}|-----------------------------|     |-----------------------------|{xhb}|\n",
    "        |{oho}| 01 | 02 | 03 | 04 | 05 | 06 |{ost}| 07 | 08 | 09 | 10 | 11 | 12 |{xha}|\n",
    "        \"\"\"\n",
    "\n",
    "        pixels = {}\n",
    "        for pos in range(1,25):\n",
    "            x = chr(ord(\"a\") + pos - 1)\n",
    "            checkers = abs(int(self.board[pos-1]))\n",
    "            pixel = self._render_player(player = 0 if self.board[pos-1] > 0 else 1)\n",
    "            for y in range(1, 6):\n",
    "                if checkers > 0:\n",
    "                    if y < 5 or checkers == 1:\n",
    "                        pixels[f\"{x}{y}\"] = (f\"  {pixel} \")\n",
    "                    else: # y == 5 and checkers > 1:\n",
    "                        pixels[f\"{x}{y}\"] = f\"({checkers}\".rjust(3) + \")\"\n",
    "                    checkers -= 1\n",
    "                else:\n",
    "                    pixels[f\"{x}{y}\"] = \"    \"\n",
    "        pixels[\"dcs\"] = f\" {self.dice[0] or ' '}:{self.dice[1] or ' '} \"\n",
    "\n",
    "        for player in [0, 1]:\n",
    "            pixel = self._render_player(player=player)\n",
    "            for idx in range(15):\n",
    "                h = pixel + \"h\" + chr(ord(\"a\") + idx)\n",
    "                pixels[h] = f\"  {pixel}  \" if self.home[player] > idx else \"     \"\n",
    "\n",
    "        pixels[\"ost\"] = \"     \"\n",
    "        pixels[\"xst\"] = \"     \"\n",
    "        if self.step == Game.Step.ROLL:\n",
    "            pixels[\"ost\" if self.pturn == 0 else \"xst\"] = \" ROL \"\n",
    "        elif self.step == Game.Step.TURN:\n",
    "            pixels[\"ost\" if self.pturn == 0 else \"xst\"] = f\" ({len([d for d in self.dice if d])}) \"\n",
    "        elif self.step == Game.Step.FINISHED:\n",
    "            pixels[\"ost\" if self.pturn == 0 else \"xst\"] = \" WIN \"\n",
    "\n",
    "        return template.format(**pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate(game: Game):\n",
    "#     p1, p2 = 0, 0\n",
    "#     for i in range(len(game.board)):\n",
    "#         if game.board[i] > 0:\n",
    "#             p1 += game.board[i]\n",
    "#         if game.board[i] < 0:\n",
    "#             p2 -= game.board[i]\n",
    "#     p1 += game.home[0]\n",
    "#     p2 += game.home[1]\n",
    "#     assert (p1, p2) == (15, 15), \"invalid # of checkers at the board\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_move(game: Game):\n",
    "#     if not game.is_finished():\n",
    "#         moves = game.get_valid_moves()\n",
    "#         if len(moves) > 0:\n",
    "#             pos, steps = random.choice(moves)\n",
    "#             game.turn(pos, steps)\n",
    "#             return True\n",
    "#         else:\n",
    "#             game.skip()\n",
    "#     return False\n",
    "\n",
    "# def auto_turn(game: Game):\n",
    "#     game.roll()\n",
    "#     for _ in range(len(game.dice)):\n",
    "#         if not random_move(game):\n",
    "#             break\n",
    "#     return game\n",
    "\n",
    "# def auto_rollout(game, turns: int = 100):\n",
    "#     for turn in range(turns):\n",
    "#         auto_turn(game)\n",
    "#         validate(game)\n",
    "#         if game.is_finished():\n",
    "#             break\n",
    "#     return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = Game(seed=42, verbose=True)\n",
    "# auto_rollout(g.start(), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_blocking_primes (__main__.GameTests) ... ok\n",
      "test_heads (__main__.GameTests) ... ok\n",
      "test_nonblocking_primes (__main__.GameTests) ... ok\n",
      "test_skips (__main__.GameTests) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x16801dbd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GameTests(unittest.TestCase):\n",
    "\n",
    "    def test_heads(self):\n",
    "        g = Game(seed=42).start()\n",
    "        g.roll(3,3).turn(1,3).turn(1,3) # < can make 2 moves from head on doubles\n",
    "        with self.assertRaises(RuntimeError):\n",
    "            g.turn(1,3)\n",
    "        g.turn(4,3).turn(4,3)\n",
    "        g.roll(1,2).turn(13,1).turn(14,2) # p2\n",
    "        g.roll(2,2).turn(1,2)\n",
    "        with self.assertRaises(RuntimeError):\n",
    "            g.turn(1, 2)\n",
    "        g.turn(3,2)\n",
    "        with self.assertRaises(RuntimeError):\n",
    "            g.turn(1, 2)\n",
    "        g.turn(5,2)\n",
    "\n",
    "    def test_skips(self):\n",
    "        g = Game(seed=42).start()\n",
    "        g.roll(6,1).turn(1, 6).turn(7, 1)\n",
    "        g.roll(3,2).turn(13, 3).turn(16, 2)\n",
    "        g.roll(2,2).turn(1, 2).turn(3, 2).turn(5, 2).turn(8, 2)\n",
    "        g.roll(6,1).turn(13,6).turn(18,1)\n",
    "        g.roll(6,6).turn(10, 6).turn(16,6).turn(1, 6) # < there's no moves for red here\n",
    "        with self.assertRaises(RuntimeError):\n",
    "            g.turn(1, 6)\n",
    "        g.skip()\n",
    "\n",
    "    def test_blocking_primes(self):\n",
    "        g = Game(seed=42).start()\n",
    "        g.roll(1,1).turn(1,1).turn(1,1).turn(2,1).turn(3,1)\n",
    "        g.roll(6,5).turn(13,6).turn(19,5)\n",
    "        g.roll(2,3).turn(1,2).turn(3,3)\n",
    "        g.roll(4,2).turn(13,4).turn(17,2)\n",
    "        g.roll(3,1).turn(1,1).turn(2,3)\n",
    "        g.roll(3,6).turn(13,3).turn(16,6)\n",
    "        g.roll(1,2)\n",
    "        with self.assertRaises(RuntimeError):\n",
    "            g.turn(1,2)\n",
    "\n",
    "    def test_nonblocking_primes(self):\n",
    "        g = Game(seed=42).start()\n",
    "        g.roll(1,1).turn(1,1).turn(1,1).turn(2,1).turn(3,1)\n",
    "        g.roll(6,5).turn(13,6).turn(19,5)\n",
    "        g.roll(2,3).turn(1,2).turn(3,3)\n",
    "        g.roll(4,2).turn(13,4).turn(17,2)\n",
    "        g.roll(3,1).turn(1,1).turn(2,3)\n",
    "        g.roll(3,6).turn(24,3).turn(3,6)\n",
    "        g.roll(1,2).turn(1,2) # < valid move, since x is ahead\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player interface & automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePlayer:\n",
    "    def play_turn(self, game: Game) -> bool:\n",
    "        pass\n",
    "\n",
    "class RandomPlayer(BasePlayer):\n",
    "    def play_turn(self, game: Game) -> bool:\n",
    "        actions = game.get_valid_moves()\n",
    "        if len(actions) > 0:\n",
    "            pos, steps = random.choice(actions)\n",
    "            game.turn(pos, steps)\n",
    "            return True\n",
    "        else:\n",
    "            game.skip()\n",
    "            return False\n",
    "\n",
    "class LazyPlayer(BasePlayer):\n",
    "    def play_turn(self, game: Game) -> bool:\n",
    "        actions = game.get_valid_moves()\n",
    "        if actions:\n",
    "            pos, steps = actions[0]\n",
    "            game.turn(pos, steps)\n",
    "            return True\n",
    "        else:\n",
    "            game.skip()\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoGame(Game):\n",
    "    def __init__(self, player1: BasePlayer | None = None, player2: BasePlayer | None = None, start: bool = True, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.auto_player1 = player1\n",
    "        self.auto_player2 = player2\n",
    "        if start:\n",
    "            self.start()\n",
    "\n",
    "    def _automate(self) -> \"AutoGame\":\n",
    "        while not self.is_finished():\n",
    "            if self.step == Game.Step.ROLL:\n",
    "                self.roll()\n",
    "            if self.step == Game.Step.TURN:\n",
    "                if not self.has_valid_moves():\n",
    "                    self.skip()\n",
    "                else:\n",
    "                    player = self.auto_player1 if (self.pturn == 0) else self.auto_player2\n",
    "                    if player:\n",
    "                        player.play_turn(self)\n",
    "                    else:\n",
    "                        break\n",
    "        return self\n",
    "   \n",
    "    def start(self, *args, **kwargs) -> \"AutoGame\":\n",
    "        super().start(*args, **kwargs)\n",
    "        return self._automate()\n",
    "\n",
    "    def turn(self, *args, **kwargs) -> \"AutoGame\":\n",
    "        super().turn(*args, **kwargs)\n",
    "        return self._automate()\n",
    "\n",
    "    def play_sequence(self, turns: List[Tuple[int, int]]) -> \"AutoGame\":\n",
    "        for (pos, steps) in turns:\n",
    "            self.turn(pos, steps)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_autoplayer (__main__.AutoGameTests) ... ok\n",
      "test_basic (__main__.AutoGameTests) ... ok\n",
      "test_blocking_primes (__main__.GameTests) ... ok\n",
      "test_heads (__main__.GameTests) ... ok\n",
      "test_nonblocking_primes (__main__.GameTests) ... ok\n",
      "test_skips (__main__.GameTests) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.018s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x168014d60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AutoGameTests(unittest.TestCase):\n",
    "\n",
    "    def test_basic(self):\n",
    "        AutoGame(seed=324).play_sequence([(1, 6), (1, 6), (13, 1), (14, 4),(7,5),(1,4),(13,1),(18,3)])\n",
    "        with self.assertRaises(RuntimeError):\n",
    "            AutoGame(seed=324).play_sequence([(1, 6), (1, 6), (13, 1), (14, 4),(7,5),(1,4),(13,1),(18,3),(4, 1)])\n",
    "\n",
    "    def test_autoplayer(self):\n",
    "        AutoGame(seed=324, player2=LazyPlayer()).play_sequence([(1,6), (1,6), (7,5), (1,4)])\n",
    "        AutoGame(seed=324, player1=LazyPlayer()).play_sequence([(13,4), (17,1)])\n",
    "        self.assertEqual(AutoGame(seed=324, player1=LazyPlayer(), player2=LazyPlayer()).is_finished(), True)\n",
    "        self.assertEqual(AutoGame(seed=324, start=False, player1=LazyPlayer(), player2=LazyPlayer()).is_finished(), False)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Result:\n",
    "    winner: int\n",
    "    turns: int\n",
    "    reward: int\n",
    "\n",
    "def summarize(game: Game) -> Result:\n",
    "    assert game.is_finished()\n",
    "    reward =  (2 if game.home[game._opponent] == 0 else 1)\n",
    "    reward *= (1 if game.pturn == 0 else -1)\n",
    "    result = Result(\n",
    "        winner = int(game.pturn == 0),\n",
    "        turns = game.t,\n",
    "        reward = reward\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def simulate(game: Game, player1: BasePlayer, player2: BasePlayer | None = None) -> Result:\n",
    "    while not game.is_finished():\n",
    "        player = player1 if (game.pturn == 0) else player2\n",
    "        if player:\n",
    "            player.play_turn(game)\n",
    "    return summarize(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize(AutoGame(player1=LazyPlayer(), player2=LazyPlayer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate(AutoGame(player2=LazyPlayer()), player1=LazyPlayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate(AutoGame(), player1=LazyPlayer(), player2=LazyPlayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1d1ee4e8f5437e9ac952156b012d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad667ec18ac14af6a425414b2a6164d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, p1=lazy, p2=lazy, start=random]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2908353821e4a6fb053a882e30a1656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, p1=lazy, p2=lazy, start=first]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c693acb12afd4d7091c455506af3b60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, p1=lazy, p2=lazy, start=second]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1134cd3aed494f8eb59da6e44bfefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, p1=lazy, p2=rand, start=random]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45732afb363473e820a30dcb7f52dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, p1=lazy, p2=rand, start=first]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc219941999a47c2a970b2477e45c55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, p1=lazy, p2=rand, start=second]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0f8faceb4a4516a36a714062da199c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, p1=rand, p2=lazy, start=random]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5ab71b91db4e9aabebd0e2108a2a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, p1=rand, p2=lazy, start=first]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd61f808d60c4342b15806359cc80887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, p1=rand, p2=lazy, start=second]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23244eb9d1b442ca37adcbec73d6683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, p1=rand, p2=rand, start=random]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285372eafd854282a41ea1cbc31ca1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, p1=rand, p2=rand, start=first]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eaeaa35f4d14da3a3d9a53aae744862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, p1=rand, p2=rand, start=second]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>start</th>\n",
       "      <th>games</th>\n",
       "      <th>wins</th>\n",
       "      <th>win_rate_lo</th>\n",
       "      <th>win_rate_mu</th>\n",
       "      <th>win_rate_hi</th>\n",
       "      <th>avg_turns</th>\n",
       "      <th>avg_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lazy</td>\n",
       "      <td>lazy</td>\n",
       "      <td>random</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.2530</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>90.85</td>\n",
       "      <td>-0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lazy</td>\n",
       "      <td>lazy</td>\n",
       "      <td>first</td>\n",
       "      <td>100</td>\n",
       "      <td>48</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>92.42</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lazy</td>\n",
       "      <td>lazy</td>\n",
       "      <td>second</td>\n",
       "      <td>100</td>\n",
       "      <td>43</td>\n",
       "      <td>0.2815</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.5785</td>\n",
       "      <td>91.19</td>\n",
       "      <td>-0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lazy</td>\n",
       "      <td>rand</td>\n",
       "      <td>random</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.8799</td>\n",
       "      <td>92.77</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lazy</td>\n",
       "      <td>rand</td>\n",
       "      <td>first</td>\n",
       "      <td>100</td>\n",
       "      <td>89</td>\n",
       "      <td>0.7961</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>92.75</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lazy</td>\n",
       "      <td>rand</td>\n",
       "      <td>second</td>\n",
       "      <td>100</td>\n",
       "      <td>76</td>\n",
       "      <td>0.6319</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.8881</td>\n",
       "      <td>93.42</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rand</td>\n",
       "      <td>lazy</td>\n",
       "      <td>random</td>\n",
       "      <td>100</td>\n",
       "      <td>56</td>\n",
       "      <td>0.4112</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>94.37</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rand</td>\n",
       "      <td>lazy</td>\n",
       "      <td>first</td>\n",
       "      <td>100</td>\n",
       "      <td>46</td>\n",
       "      <td>0.3106</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.6094</td>\n",
       "      <td>93.48</td>\n",
       "      <td>-0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rand</td>\n",
       "      <td>lazy</td>\n",
       "      <td>second</td>\n",
       "      <td>100</td>\n",
       "      <td>51</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>93.15</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rand</td>\n",
       "      <td>rand</td>\n",
       "      <td>random</td>\n",
       "      <td>100</td>\n",
       "      <td>48</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>96.39</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rand</td>\n",
       "      <td>rand</td>\n",
       "      <td>first</td>\n",
       "      <td>100</td>\n",
       "      <td>57</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.7185</td>\n",
       "      <td>95.95</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rand</td>\n",
       "      <td>rand</td>\n",
       "      <td>second</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>95.98</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p1    p2   start  games  wins  win_rate_lo  win_rate_mu  win_rate_hi  \\\n",
       "0   lazy  lazy  random    100    40       0.2530         0.40       0.5470   \n",
       "1   lazy  lazy   first    100    48       0.3300         0.48       0.6300   \n",
       "2   lazy  lazy  second    100    43       0.2815         0.43       0.5785   \n",
       "3   lazy  rand  random    100    75       0.6201         0.75       0.8799   \n",
       "4   lazy  rand   first    100    89       0.7961         0.89       0.9839   \n",
       "5   lazy  rand  second    100    76       0.6319         0.76       0.8881   \n",
       "6   rand  lazy  random    100    56       0.4112         0.56       0.7088   \n",
       "7   rand  lazy   first    100    46       0.3106         0.46       0.6094   \n",
       "8   rand  lazy  second    100    51       0.3600         0.51       0.6600   \n",
       "9   rand  rand  random    100    48       0.3300         0.48       0.6300   \n",
       "10  rand  rand   first    100    57       0.4215         0.57       0.7185   \n",
       "11  rand  rand  second    100    50       0.3500         0.50       0.6500   \n",
       "\n",
       "    avg_turns  avg_reward  \n",
       "0       90.85       -0.14  \n",
       "1       92.42        0.09  \n",
       "2       91.19       -0.08  \n",
       "3       92.77        0.73  \n",
       "4       92.75        1.14  \n",
       "5       93.42        0.78  \n",
       "6       94.37        0.12  \n",
       "7       93.48       -0.06  \n",
       "8       93.15        0.02  \n",
       "9       96.39       -0.07  \n",
       "10      95.95        0.18  \n",
       "11      95.98       -0.02  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "games = 100\n",
    "\n",
    "results = []\n",
    "for p1, p2, start in tqdm.tqdm(list(itertools.product([\"lazy\", \"rand\"], [\"lazy\", \"rand\"], [\"random\", \"first\", \"second\"])), leave=False):\n",
    "    exp_info = {\"p1\": p1, \"p2\": p2, \"start\": start}\n",
    "    player1 = LazyPlayer() if p1 == \"lazy\" else RandomPlayer()\n",
    "    player2 = LazyPlayer() if p2 == \"lazy\" else RandomPlayer()\n",
    "    start_args = [6,6] if start == \"random\" else ([6,1] if start == \"first\" else [1,6])\n",
    "    sims = pd.DataFrame([summarize(AutoGame(player1, player2, False).start(*start_args)).__dict__ for _ in tqdm.trange(games, postfix=exp_info, leave=False)])\n",
    "    exp_info[\"games\"] = games\n",
    "    exp_info[\"wins\"] = sims[\"winner\"].sum()\n",
    "    wins_mu = exp_info[\"wins\"] / exp_info[\"games\"]\n",
    "    wins_sd = round(math.sqrt(exp_info[\"games\"] * wins_mu * (1 - wins_mu)), 2)\n",
    "    exp_info[\"win_rate_lo\"] = (exp_info[\"wins\"] - wins_sd*3) / exp_info[\"games\"]\n",
    "    exp_info[\"win_rate_mu\"] = wins_mu\n",
    "    exp_info[\"win_rate_hi\"] = (exp_info[\"wins\"] + wins_sd*3) / exp_info[\"games\"]\n",
    "    exp_info[\"avg_turns\"] = sims[\"turns\"].mean()\n",
    "    exp_info[\"avg_reward\"] = sims[\"reward\"].mean()\n",
    "    results.append(exp_info)\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting bias is observed here, as lazy policy seems to have a significant advantage when played against random.  \n",
    "Bias only shows up if it plays as player1, but disappears if it plays as player2\n",
    "\n",
    "TODO: Figure out what's going on here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model with Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReplaySample:\n",
    "    state_action: torch.Tensor\n",
    "    reward: int = 0\n",
    "    next_state_actions: torch.Tensor | None = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size: 1_000_000):\n",
    "        self.buffer = [None] * size\n",
    "        self.insert_ptr = 0\n",
    "        self.upper_bound = 0\n",
    "\n",
    "    def add(self, sample: ReplaySample):\n",
    "        self.buffer[self.insert_ptr] = sample\n",
    "        self.insert_ptr = self.insert_ptr+1 if self.insert_ptr+1 < len(self.buffer) else 0\n",
    "        self.upper_bound = max(self.insert_ptr, self.upper_bound)\n",
    "    \n",
    "    def sample(self, k: int = 1) -> List[ReplaySample]:\n",
    "        return random.choices(self.buffer[:self.upper_bound], k=k)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> ReplaySample:\n",
    "        if index < self.upper_bound:\n",
    "            return self.buffer[index]\n",
    "        raise IndexError()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPolicy(BasePlayer):\n",
    "    def __init__(\n",
    "            self, \n",
    "            layers=[32, 64], \n",
    "            device=\"cpu\", \n",
    "            training: bool = True,\n",
    "            replay_buffer_size: int = 1_000_000\n",
    "        ):\n",
    "        self.device = device\n",
    "        network = []\n",
    "        network.append(torch.nn.Linear(31, layers[0]))\n",
    "        network.append(torch.nn.ReLU())\n",
    "        for idx in range(1, len(layers)):\n",
    "            network.append(torch.nn.Linear(layers[idx-1], layers[idx]))\n",
    "            network.append(torch.nn.ReLU())\n",
    "        network.append(torch.nn.Linear(layers[-1], 1))\n",
    "        self.q_network = torch.nn.Sequential(*network).to(self.device)\n",
    "        self.t_network = torch.nn.Sequential(*network).to(self.device)\n",
    "        self.training = training\n",
    "        self.replay_buffer = ReplayBuffer(size=replay_buffer_size)\n",
    "        # self.q_network.apply(self._init_weights)\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters())\n",
    "        self.prev_state_action = None\n",
    "        self.gamma = 1\n",
    "        self.lr = 0.001\n",
    "        self.grad_clip = 10\n",
    "        self.soft_epsilon = 0.05\n",
    "        self._sync_networks()\n",
    "\n",
    "    # @staticmethod\n",
    "    # def _init_weights(m):\n",
    "    #     if hasattr(m, \"weight\"):\n",
    "    #         torch.nn.init.xavier_uniform_(m.weight, gain=2 ** (1.0 / 2))\n",
    "    #     if hasattr(m, \"bias\"):\n",
    "    #         torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _sync_networks(self) -> None:\n",
    "        logger.debug(\"syncing weights of t-network\")\n",
    "        self.t_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def _encode_state(self, game: Game) -> torch.Tensor:\n",
    "        dice = game.dice.copy()\n",
    "        if len(dice) < 4:\n",
    "            dice += [0] * (4 - len(dice))\n",
    "        return torch.concat([\n",
    "            torch.tensor(game.board),\n",
    "            torch.tensor(dice),\n",
    "            torch.tensor([game.head_moves])\n",
    "        ]).to(self.device).float()\n",
    "\n",
    "    def _encode_actions(self, actions: List[Tuple[int, int]]) -> torch.Tensor:\n",
    "        if not actions:\n",
    "            actions = [(-1,-1)]\n",
    "        return torch.tensor(actions).to(self.device).float()\n",
    "    \n",
    "    def _encode_state_actions(self, game: Game) -> torch.Tensor:\n",
    "        return self._concat_state_actions(\n",
    "            self._encode_state(game),\n",
    "            self._encode_actions(game.get_valid_moves())\n",
    "        )\n",
    "\n",
    "    def _concat_state_actions(self, state: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        actions = actions.view(-1, 2)\n",
    "        return torch.concat([\n",
    "                state.unsqueeze(dim=0).broadcast_to((actions.shape[0], -1)),\n",
    "                actions\n",
    "            ], dim=1)\n",
    "   \n",
    "    def _sample_batch(self, batch_size: int = 32):\n",
    "        rewards = []\n",
    "        curr_state_actions = []\n",
    "        next_state_actions = []\n",
    "        next_state_actions_idx = []\n",
    "        for sample_id, sample in enumerate(self.replay_buffer.sample(k=batch_size)):\n",
    "            rewards.append(sample.reward)\n",
    "            curr_state_actions.append(sample.state_action)\n",
    "            if sample.next_state_actions is not None:\n",
    "                next_state_actions.append(sample.next_state_actions)\n",
    "                next_state_actions_idx.extend([sample_id] * sample.next_state_actions.shape[0])\n",
    "\n",
    "        rewards = torch.tensor(rewards).float().to(self.device)\n",
    "        curr_state_actions = torch.vstack(curr_state_actions)\n",
    "        next_state_actions = torch.vstack(next_state_actions)\n",
    "        next_state_actions_idx = torch.tensor(next_state_actions_idx, dtype=torch.long).to(self.device)\n",
    "        assert next_state_actions.shape[0] == next_state_actions_idx.shape[0]\n",
    "        return curr_state_actions, rewards, next_state_actions, next_state_actions_idx\n",
    "    \n",
    "    def _calc_loss(self, q_scores, t_scores, sample_ids, rewards) -> torch.Tensor:\n",
    "        # t_scores_max = rewards.scatter_reduce(dim=0, index=sample_ids, src=t_scores.squeeze(), reduce=\"max\", include_self=False)\n",
    "        t_scores_max, t_score_idx = torch_scatter.scatter_max(t_scores.squeeze(), index=sample_ids, dim=0)\n",
    "        td_target = rewards + self.gamma * t_scores_max\n",
    "        td_error = td_target - q_scores.squeeze()\n",
    "        return td_error.pow(2).mean()\n",
    "    \n",
    "    def _train_step(self, batch_size: int = 32) -> None:\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            logger.warning(\"not enough samples in replay buffer, skipping train step\")\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "    \n",
    "        curr_sa, rewards, next_sa, sample_ids  = self._sample_batch(batch_size)\n",
    "\n",
    "        q_scores = self.q_network(curr_sa)\n",
    "        with torch.no_grad():\n",
    "            t_scores = self.t_network(next_sa)\n",
    "\n",
    "        loss = self._calc_loss(q_scores, t_scores, sample_ids, rewards)\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), self.grad_clip)\n",
    "            \n",
    "        for group in self.optimizer.param_groups:\n",
    "            group[\"lr\"] = self.lr\n",
    "        \n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), grad_norm.item()\n",
    "\n",
    "\n",
    "    def play_turn(self, game: Game):\n",
    "        valid_moves = list(game.get_valid_moves())\n",
    "        if not valid_moves:\n",
    "            game.skip()\n",
    "            return False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state_actions = self._encode_state_actions(game)\n",
    "\n",
    "            if self.training and self.prev_state_action is not None:\n",
    "                self.replay_buffer.add(ReplaySample(state_action=self.prev_state_action, reward=self._calc_reward(game), next_state_actions=state_actions))\n",
    "                self.prev_state_action = None\n",
    "        \n",
    "            scores = self.q_network(state_actions).squeeze()\n",
    "            action_idx = scores.argmax(dim=-1).item()\n",
    "            if np.random.random() < self.soft_epsilon:\n",
    "                action_idx = random.randint(0, len(valid_moves)-1)\n",
    "            self.prev_state_action = state_actions[action_idx]\n",
    "\n",
    "            game.turn(*valid_moves[action_idx])\n",
    "\n",
    "            if game.is_finished() and self.training and self.prev_state_action is not None:\n",
    "                self.replay_buffer.add(ReplaySample(state_action=self.prev_state_action, reward=self._calc_reward(game)))\n",
    "                self.prev_state_action = None\n",
    "\n",
    "            # self._train_step()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _calc_reward(self, game: Game) -> int:\n",
    "        reward = 0\n",
    "        if game.is_finished():\n",
    "            reward  = (2 if game.home[game._opponent] == 0 else 1)\n",
    "            reward *= (1 if game.pturn == 0 else -1)\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def practice(\n",
    "        policy: QPolicy, \n",
    "        games: int = 100,\n",
    "        train_every: int = 1, \n",
    "        sync_every: int = 100,\n",
    "        batch_size = 32,\n",
    "        show_progress : bool = False, \n",
    "        gamma: float = 0.99, \n",
    "        lr: float = 0.001,\n",
    "        grad_clip: float = 10,\n",
    "        soft_epsilon: float = 0\n",
    "    ):\n",
    "    policy.training = True\n",
    "    policy.gamma = gamma\n",
    "    policy.lr = lr\n",
    "    policy.grad_clip = grad_clip\n",
    "    policy.soft_epsilon = soft_epsilon\n",
    "\n",
    "    loss_vals = []\n",
    "    grad_vals = []\n",
    "    for game_id in (tqdm.trange(games, leave=False, desc=\"practicing\") if show_progress else range(games)):\n",
    "        simulate(AutoGame(), player1=policy, player2=RandomPlayer())\n",
    "        if game_id % train_every == 0:\n",
    "            loss, grad = policy._train_step(batch_size=batch_size)\n",
    "            loss_vals.append(loss)\n",
    "            grad_vals.append(grad)\n",
    "        if game_id % sync_every == 0:\n",
    "            policy._sync_networks()\n",
    "    return sum(loss_vals) / len(loss_vals), sum(grad_vals) / len(grad_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_name: str, policy: QPolicy, games: int = 100):\n",
    "    prev_training = policy.training\n",
    "    policy.training = False\n",
    "\n",
    "    results = []\n",
    "    sims = pd.DataFrame([simulate(AutoGame(player2=RandomPlayer()), player1=policy).__dict__ for _ in tqdm.trange(games, leave=False, desc=\"evaluating\")])\n",
    "    exp_info = {\"model\": model_name, \"p2\": \"random\", \"start\": \"random\"}\n",
    "    exp_info[\"games\"] = games\n",
    "    exp_info[\"wins\"] = sims[\"winner\"].sum()\n",
    "    wins_mu = exp_info[\"wins\"] / exp_info[\"games\"]\n",
    "    wins_sd = round(math.sqrt(exp_info[\"games\"] * wins_mu * (1 - wins_mu)), 2)\n",
    "    exp_info[\"win_rate_lo\"] = (exp_info[\"wins\"] - wins_sd*3) / exp_info[\"games\"]\n",
    "    exp_info[\"win_rate_mu\"] = wins_mu\n",
    "    exp_info[\"win_rate_hi\"] = (exp_info[\"wins\"] + wins_sd*3) / exp_info[\"games\"]\n",
    "    exp_info[\"avg_turns\"] = sims[\"turns\"].mean()\n",
    "    exp_info[\"avg_reward\"] = sims[\"reward\"].mean()\n",
    "    results.append(exp_info)\n",
    "\n",
    "    policy.training = prev_training\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_loop(\n",
    "    policy: QPolicy,\n",
    "    epochs: int = 1000,\n",
    "    practice_games: int = 1000,\n",
    "    batch_size: int = 32,\n",
    "    eval_games: int = 100,\n",
    "    sync_every: int = 50,\n",
    "    **kwargs\n",
    "):\n",
    "    epoch_pbar = tqdm.trange(1, epochs+1, desc=\"train/eval epochs\")\n",
    "    results = []\n",
    "    result = evaluate(f\"untrained\", policy, games=eval_games).loc[0].to_dict()\n",
    "    epoch_pbar.set_postfix({\"win_rate\": result[\"win_rate_mu\"], \"avg_reward\": result[\"avg_reward\"]})\n",
    "    logger.info(f\"untrained: win_rate={result['win_rate_mu']:.4%}, avg_reward={result['avg_reward']:.2f}\")\n",
    "    results.append(result)\n",
    "    for epoch_id in epoch_pbar:\n",
    "        avg_loss, avg_grad = practice(policy, games=practice_games, train_every=1, sync_every=sync_every, batch_size=batch_size, show_progress=True, **kwargs)\n",
    "        epoch_pbar.set_postfix({\"win_rate\": result[\"win_rate_mu\"], \"avg_reward\": result[\"avg_reward\"], \"avg_loss\": avg_loss, \"avg_grad\": avg_grad})\n",
    "        result = evaluate(f\"epoch-{epoch_id}\", policy, games=eval_games).loc[0].to_dict()\n",
    "        result[\"avg_loss\"] = avg_loss\n",
    "        result[\"avg_grad\"] = avg_grad\n",
    "        results.append(result)\n",
    "        epoch_pbar.set_postfix({\"win_rate\": result[\"win_rate_mu\"], \"avg_reward\": result[\"avg_reward\"], \"avg_loss\": avg_loss, \"avg_grad\": avg_grad})\n",
    "        logger.info(f\"epoch={epoch_id}: win_rate={result['win_rate_mu']:.4%}, avg_reward={result['avg_reward']:.2f}, {avg_loss=:.4f}, {avg_grad=:.4f}\")\n",
    "    results = pd.DataFrame(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_player = QPolicy(layers=[32, 64, 32], device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_args = {\"epochs\": 10, \"practice_games\": 5000, \"batch_size\": 1000, \"eval_games\": 100, \"sync_every\": 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4c295c97524007b131e207d4a7d39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/eval epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194f05481aa744ec902bbf2b8f7215eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-10 22:58:35,494 - research - INFO - untrained: win_rate=34.0000%, avg_reward=-0.47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a146fd9ae2f0417c83a6ebec5e319722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-10 22:58:35,522 - research - WARNING - not enough samples in replay buffer, skipping train step\n",
      "2024-12-10 22:58:35,569 - research - WARNING - not enough samples in replay buffer, skipping train step\n",
      "2024-12-10 22:58:35,596 - research - WARNING - not enough samples in replay buffer, skipping train step\n",
      "2024-12-10 22:58:35,623 - research - WARNING - not enough samples in replay buffer, skipping train step\n",
      "2024-12-10 22:58:35,647 - research - WARNING - not enough samples in replay buffer, skipping train step\n",
      "2024-12-10 22:58:35,672 - research - WARNING - not enough samples in replay buffer, skipping train step\n",
      "2024-12-10 22:58:35,696 - research - WARNING - not enough samples in replay buffer, skipping train step\n",
      "2024-12-10 22:58:35,723 - research - WARNING - not enough samples in replay buffer, skipping train step\n",
      "2024-12-10 22:58:35,748 - research - WARNING - not enough samples in replay buffer, skipping train step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_eval_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_player\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoft_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_eval_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 17\u001b[0m, in \u001b[0;36mtrain_eval_loop\u001b[0;34m(policy, epochs, practice_games, batch_size, eval_games, sync_every, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_id \u001b[38;5;129;01min\u001b[39;00m epoch_pbar:\n\u001b[0;32m---> 17\u001b[0m     avg_loss, avg_grad \u001b[38;5;241m=\u001b[39m \u001b[43mpractice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpractice_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msync_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     epoch_pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin_rate_mu\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_grad})\n\u001b[1;32m     19\u001b[0m     result \u001b[38;5;241m=\u001b[39m evaluate(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, policy, games\u001b[38;5;241m=\u001b[39meval_games)\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "Cell \u001b[0;32mIn[23], line 22\u001b[0m, in \u001b[0;36mpractice\u001b[0;34m(policy, games, train_every, sync_every, batch_size, show_progress, gamma, lr, grad_clip, soft_epsilon)\u001b[0m\n\u001b[1;32m     20\u001b[0m grad_vals \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m game_id \u001b[38;5;129;01min\u001b[39;00m (tqdm\u001b[38;5;241m.\u001b[39mtrange(games, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpracticing\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m show_progress \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(games)):\n\u001b[0;32m---> 22\u001b[0m     \u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAutoGame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRandomPlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m game_id \u001b[38;5;241m%\u001b[39m train_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m         loss, grad \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39m_train_step(batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m, in \u001b[0;36msimulate\u001b[0;34m(game, player1, player2)\u001b[0m\n\u001b[1;32m     20\u001b[0m     player \u001b[38;5;241m=\u001b[39m player1 \u001b[38;5;28;01mif\u001b[39;00m (game\u001b[38;5;241m.\u001b[39mpturn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m player2\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m player:\n\u001b[0;32m---> 22\u001b[0m         \u001b[43mplayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summarize(game)\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mRandomPlayer.play_turn\u001b[0;34m(self, game)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(actions) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      9\u001b[0m     pos, steps \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(actions)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mturn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m, in \u001b[0;36mAutoGame.turn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mturn\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoGame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mturn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_automate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m, in \u001b[0;36mAutoGame._automate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroll()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m==\u001b[39m Game\u001b[38;5;241m.\u001b[39mStep\u001b[38;5;241m.\u001b[39mTURN:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_valid_moves\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 152\u001b[0m, in \u001b[0;36mGame.has_valid_moves\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_valid_moves\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_valid_moves\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 148\u001b[0m, in \u001b[0;36mGame.get_valid_moves\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_valid_moves\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_moves \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_moves \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enum_valid_moves\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_moves\n",
      "Cell \u001b[0;32mIn[6], line 143\u001b[0m, in \u001b[0;36mGame._enum_valid_moves\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_checkers(pos, player\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_player):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m steps \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_valid_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    144\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m (pos, steps)\n",
      "Cell \u001b[0;32mIn[6], line 134\u001b[0m, in \u001b[0;36mGame._is_valid_move\u001b[0;34m(self, pos, steps)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_valid_move\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos: \u001b[38;5;28mint\u001b[39m, steps: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[6], line 111\u001b[0m, in \u001b[0;36mGame._check_move\u001b[0;34m(self, pos, steps)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m pos \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid position\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_checkers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cur_player\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno checkers at position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_checkers(dst_pos, player\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opponent):\n",
      "Cell \u001b[0;32mIn[6], line 72\u001b[0m, in \u001b[0;36mGame._has_checkers\u001b[0;34m(self, pos, player)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_has_checkers\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos: \u001b[38;5;28mint\u001b[39m, player: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_checkers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 76\u001b[0m, in \u001b[0;36mGame._get_checkers\u001b[0;34m(self, pos, player)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_checkers\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos: \u001b[38;5;28mint\u001b[39m, player: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     75\u001b[0m     player \u001b[38;5;241m=\u001b[39m player \u001b[38;5;28;01mif\u001b[39;00m player \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpturn\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_user_sign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard[pos\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m, in \u001b[0;36mGame._get_user_sign\u001b[0;34m(self, player)\u001b[0m\n\u001b[1;32m     43\u001b[0m     player \u001b[38;5;241m=\u001b[39m player \u001b[38;5;28;01mif\u001b[39;00m player \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpturn\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m player \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m13\u001b[39m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_user_sign\u001b[39m(\u001b[38;5;28mself\u001b[39m, player: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     47\u001b[0m     player \u001b[38;5;241m=\u001b[39m player \u001b[38;5;28;01mif\u001b[39;00m player \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpturn\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m player \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_eval_loop(nn_player, lr=0.001, gamma=0.99, grad_clip=10, soft_epsilon=0.5, **train_eval_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_loop(nn_player, lr=0.0005, gamma=0.99, grad_clip=10, soft_epsilon=0.25, **train_eval_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_loop(nn_player, lr=0.00025, gamma=0.99, grad_clip=10, soft_epsilon=0.10, **train_eval_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_loop(nn_player, lr=0.00015, gamma=0.99, grad_clip=10, soft_epsilon=0.05, **train_eval_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_loop(nn_player, lr=0.00010, gamma=0.99, grad_clip=10, soft_epsilon=0.01, **train_eval_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_loop(nn_player, lr=0.00005, gamma=0.99, grad_clip=10, soft_epsilon=0.00, **train_eval_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with scatter & reduce:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scatter_reduce implementation fails with the following error:\n",
    "\n",
    "NotImplementedError: The operator 'aten::scatter_reduce.two_out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_score = nn_player.q_network(curr_state_actions)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     t_score = nn_player.t_network(next_state_actions).squeeze()\n",
    "#     t_score_max = torch.zeros_like(rewards).scatter_reduce(0, index=next_state_actions_idx, src=t_score, reduce=\"max\", include_self=False)\n",
    "    \n",
    "# t_score_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_score = nn_player.q_network(curr_state_actions)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     t_score = nn_player.t_network(next_state_actions).squeeze()\n",
    "#     t_score_max, t_score_idx = torch_scatter.scatter_max(t_score, index=next_state_actions_idx, dim=0)\n",
    "    \n",
    "# t_score_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_score, max_score_idx = torch_scatter.scatter_max(score, index=next_state_actions_idx, dim=0)\n",
    "# max_score, max_score_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.zeros_like(rewards).scatter_reduce(0, index=next_state_actions_idx, src=score.squeeze(), reduce=\"max\", include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_score, max_score_idx = torch_scatter.scatter_max(score, index=next_state_actions_idx, dim=0)\n",
    "# max_score, max_score_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.arange(24).view(-1,6).long()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = data.float().sum(dim=1)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_score, max_score_idx = torch_scatter.scatter_max(score, index=torch.tensor([0,0,0,1]), dim=0)\n",
    "# max_score, max_score_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[max_score_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XCS234",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
