{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, str(pathlib.Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.logging\n",
    "utils.logging.setup(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backgammon.game import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model with Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReplaySample:\n",
    "    state_action: torch.Tensor\n",
    "    reward: int = 0\n",
    "    next_state_actions: torch.Tensor | None = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size: 1_000_000):\n",
    "        self.buffer = [None] * size\n",
    "        self.insert_ptr = 0\n",
    "        self.upper_bound = 0\n",
    "\n",
    "    def add(self, sample: ReplaySample):\n",
    "        self.buffer[self.insert_ptr] = sample\n",
    "        self.insert_ptr = self.insert_ptr+1 if self.insert_ptr+1 < len(self.buffer) else 0\n",
    "        self.upper_bound = max(self.insert_ptr, self.upper_bound)\n",
    "    \n",
    "    def sample(self, k: int = 1) -> List[ReplaySample]:\n",
    "        return random.choices(self.buffer[:self.upper_bound], k=k)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> ReplaySample:\n",
    "        if index < self.upper_bound:\n",
    "            return self.buffer[index]\n",
    "        raise IndexError()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGPolicy(BasePlayer):\n",
    "    def __init__(\n",
    "            self, \n",
    "            device=\"cpu\", \n",
    "            replay_buffer_size: int = 1_000_000\n",
    "        ):\n",
    "        self.device = device\n",
    "        self.replay_buffer = ReplayBuffer(size=replay_buffer_size)\n",
    "\n",
    "    def _encode_state(self, game: Game) -> torch.Tensor:\n",
    "        dice = game.dice.copy()\n",
    "        if len(dice) < 4:\n",
    "            dice += [0] * (4 - len(dice))\n",
    "        return torch.concat([\n",
    "            torch.tensor(game.board),\n",
    "            torch.tensor(dice),\n",
    "            torch.tensor([game.head_moves])\n",
    "        ]).to(self.device).float()\n",
    "\n",
    "    def _encode_actions(self, actions: List[Tuple[int, int]]) -> torch.Tensor:\n",
    "        if not actions:\n",
    "            actions = [(-1,-1)]\n",
    "        return torch.tensor(actions).to(self.device).float()\n",
    "   \n",
    "    def _sample_batch(self, batch_size: int = 32):\n",
    "        pass\n",
    "    \n",
    "    def _calc_loss(self, q_scores, t_scores, sample_ids, rewards) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def get_returns(self, rewards) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def get_advantages(self, states: torch.Tensor, returns: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            advantages = (returns - self.baseline_network(states))\n",
    "        return advantages\n",
    "    \n",
    "    def _update_policy(self, batch_size: int = 32) -> None:\n",
    "        self.optimizer.zero_grad()\n",
    "        states, actions, rewards = self._sample_batch(batch_size)\n",
    "        returns = self.get_returns(rewards)\n",
    "        advantages = self.get_advantages(returns, states)\n",
    "\n",
    "        ## advantages\n",
    "        distribution = torch.distributions.Categorical(logits=self.policy(states))\n",
    "        loss = (-distribution.log_prob(actions) * advantages).mean()\n",
    "        loss.backward()        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "\n",
    "    def _update_baseline(self, states, returns):\n",
    "        self.optimizer.zero_grad()\n",
    "        values = self.baseline_network(states)\n",
    "        advantages = (returns - values)\n",
    "        loss = advantages.pow(2).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    def play_turn(self, game: Game):\n",
    "        valid_moves = list(game.get_valid_moves())\n",
    "        if not valid_moves:\n",
    "            game.skip()\n",
    "            return False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state_actions = self._encode_state_actions(game)\n",
    "\n",
    "            if self.training and self.prev_state_action is not None:\n",
    "                self.replay_buffer.add(ReplaySample(state_action=self.prev_state_action, reward=self._calc_reward(game), next_state_actions=state_actions))\n",
    "                self.prev_state_action = None\n",
    "        \n",
    "            if game._randfloat() < self.soft_epsilon:\n",
    "                action_idx = game._randint(0, len(valid_moves)-1)\n",
    "            else:\n",
    "                scores = self.q_network(state_actions).squeeze()\n",
    "                action_idx = scores.argmax(dim=-1).item()\n",
    "\n",
    "            self.prev_state_action = state_actions[action_idx]\n",
    "\n",
    "            game.turn(*valid_moves[action_idx])\n",
    "\n",
    "            if game.is_finished() and self.training and self.prev_state_action is not None:\n",
    "                self.replay_buffer.add(ReplaySample(state_action=self.prev_state_action, reward=self._calc_reward(game)))\n",
    "                self.prev_state_action = None\n",
    "\n",
    "            # self._train_step()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _calc_reward(self, game: Game) -> int:\n",
    "        reward = 0\n",
    "        if game.is_finished():\n",
    "            reward  = (2 if game.home[game._opponent] == 0 else 1)\n",
    "            reward *= (1 if game.pturn == 0 else -1)\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def practice(\n",
    "        policy: QPolicy, \n",
    "        games: int = 100,\n",
    "        train_every: int = 1, \n",
    "        sync_every: int = 100,\n",
    "        batch_size = 32,\n",
    "        show_progress : bool = False, \n",
    "        gamma: float = 0.99, \n",
    "        lr: float = 0.001,\n",
    "        grad_clip: float = 10,\n",
    "        soft_epsilon: float = 0\n",
    "    ):\n",
    "    policy.training = True\n",
    "    policy.gamma = gamma\n",
    "    policy.lr = lr\n",
    "    policy.grad_clip = grad_clip\n",
    "    policy.soft_epsilon = soft_epsilon\n",
    "\n",
    "    loss_vals = []\n",
    "    grad_vals = []\n",
    "    for game_id in (tqdm.trange(games, leave=False, desc=\"practicing\") if show_progress else range(games)):\n",
    "        simulate(AutoGame(), player1=policy, player2=RandomPlayer())\n",
    "        if game_id % train_every == 0:\n",
    "            loss, grad = policy._train_step(batch_size=batch_size)\n",
    "            loss_vals.append(loss)\n",
    "            grad_vals.append(grad)\n",
    "        if game_id % sync_every == 0:\n",
    "            policy._sync_networks()\n",
    "    return sum(loss_vals) / len(loss_vals), sum(grad_vals) / len(grad_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_name: str, policy: QPolicy, games: int = 100):\n",
    "    prev_training = policy.training\n",
    "    policy.training = False\n",
    "\n",
    "    results = []\n",
    "    sims = pd.DataFrame([simulate(AutoGame(player2=RandomPlayer()), player1=policy).__dict__ for _ in tqdm.trange(games, leave=False, desc=\"evaluating\")])\n",
    "    exp_info = {\"model\": model_name, \"p2\": \"random\", \"start\": \"random\"}\n",
    "    exp_info[\"games\"] = games\n",
    "    exp_info[\"wins\"] = sims[\"winner\"].sum()\n",
    "    wins_mu = exp_info[\"wins\"] / exp_info[\"games\"]\n",
    "    wins_sd = round(math.sqrt(exp_info[\"games\"] * wins_mu * (1 - wins_mu)), 2)\n",
    "    exp_info[\"win_rate_lo\"] = (exp_info[\"wins\"] - wins_sd*3) / exp_info[\"games\"]\n",
    "    exp_info[\"win_rate_mu\"] = wins_mu\n",
    "    exp_info[\"win_rate_hi\"] = (exp_info[\"wins\"] + wins_sd*3) / exp_info[\"games\"]\n",
    "    exp_info[\"avg_turns\"] = sims[\"turns\"].mean()\n",
    "    exp_info[\"avg_reward\"] = sims[\"reward\"].mean()\n",
    "    results.append(exp_info)\n",
    "\n",
    "    policy.training = prev_training\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_loop(\n",
    "    policy: QPolicy,\n",
    "    epochs: int = 1000,\n",
    "    practice_games: int = 1000,\n",
    "    batch_size: int = 32,\n",
    "    eval_games: int = 100,\n",
    "    sync_every: int = 50,\n",
    "    **kwargs\n",
    "):\n",
    "    epoch_pbar = tqdm.trange(1, epochs+1, desc=\"train/eval epochs\")\n",
    "    results = []\n",
    "    result = evaluate(f\"untrained\", policy, games=eval_games).loc[0].to_dict()\n",
    "    epoch_pbar.set_postfix({\"win_rate\": result[\"win_rate_mu\"], \"avg_reward\": result[\"avg_reward\"]})\n",
    "    logger.info(f\"untrained: win_rate={result['win_rate_mu']:.4%}, avg_reward={result['avg_reward']:.2f}\")\n",
    "    results.append(result)\n",
    "    for epoch_id in epoch_pbar:\n",
    "        avg_loss, avg_grad = practice(policy, games=practice_games, train_every=1, sync_every=sync_every, batch_size=batch_size, show_progress=True, **kwargs)\n",
    "        epoch_pbar.set_postfix({\"win_rate\": result[\"win_rate_mu\"], \"avg_reward\": result[\"avg_reward\"], \"avg_loss\": avg_loss, \"avg_grad\": avg_grad})\n",
    "        result = evaluate(f\"epoch-{epoch_id}\", policy, games=eval_games).loc[0].to_dict()\n",
    "        result[\"avg_loss\"] = avg_loss\n",
    "        result[\"avg_grad\"] = avg_grad\n",
    "        results.append(result)\n",
    "        epoch_pbar.set_postfix({\"win_rate\": result[\"win_rate_mu\"], \"avg_reward\": result[\"avg_reward\"], \"avg_loss\": avg_loss, \"avg_grad\": avg_grad})\n",
    "        logger.info(f\"epoch={epoch_id}: win_rate={result['win_rate_mu']:.4%}, avg_reward={result['avg_reward']:.2f}, {avg_loss=:.4f}, {avg_grad=:.4f}\")\n",
    "    results = pd.DataFrame(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_player = QPolicy(layers=[128, 512, 128], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_args = {\"epochs\": 100, \"practice_games\": 1000, \"batch_size\": 100, \"eval_games\": 100, \"sync_every\": 999}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3bd9c613e243c0a5a20595d69cf1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/eval epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428e08ade1104ebcb15a07dc2314c4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:20:07,111 - backgammon - INFO - untrained: win_rate=88.0000%, avg_reward=1.09\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d19a058a144f57af8fa74b6ded6559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_eval_loop(nn_player, lr=0.00001, gamma=0.99, grad_clip=3, soft_epsilon=0.1, **train_eval_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with scatter & reduce:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executed on MPS, scatter_reduce implementation fails with the following error:\n",
    "\n",
    "NotImplementedError: The operator 'aten::scatter_reduce.two_out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_score = nn_player.q_network(curr_state_actions)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     t_score = nn_player.t_network(next_state_actions).squeeze()\n",
    "#     t_score_max = torch.zeros_like(rewards).scatter_reduce(0, index=next_state_actions_idx, src=t_score, reduce=\"max\", include_self=False)\n",
    "    \n",
    "# t_score_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this implementation fails on MPS with another error:\n",
    "\n",
    "RuntimeError: src.device().is_cpu() INTERNAL ASSERT FAILED at \"csrc/cpu/scatter_cpu.cpp\":11, please report a bug to PyTorch. src must be CPU tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_score = nn_player.q_network(curr_state_actions)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     t_score = nn_player.t_network(next_state_actions).squeeze()\n",
    "#     t_score_max, t_score_idx = torch_scatter.scatter_max(t_score, index=next_state_actions_idx, dim=0)\n",
    "    \n",
    "# t_score_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_score, max_score_idx = torch_scatter.scatter_max(score, index=next_state_actions_idx, dim=0)\n",
    "# max_score, max_score_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.zeros_like(rewards).scatter_reduce(0, index=next_state_actions_idx, src=score.squeeze(), reduce=\"max\", include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_score, max_score_idx = torch_scatter.scatter_max(score, index=next_state_actions_idx, dim=0)\n",
    "# max_score, max_score_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.arange(24).view(-1,6).long()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = data.float().sum(dim=1)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_score, max_score_idx = torch_scatter.scatter_max(score, index=torch.tensor([0,0,0,1]), dim=0)\n",
    "# max_score, max_score_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[max_score_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XCS234",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
