{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, str(pathlib.Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.logging\n",
    "utils.logging.setup(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backgammon.game import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model with Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReplaySample:\n",
    "    state_action: torch.Tensor\n",
    "    reward: int = 0\n",
    "    next_state_actions: torch.Tensor | None = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size: 1_000_000):\n",
    "        self.buffer = [None] * size\n",
    "        self.insert_ptr = 0\n",
    "        self.upper_bound = 0\n",
    "\n",
    "    def add(self, sample: ReplaySample):\n",
    "        self.buffer[self.insert_ptr] = sample\n",
    "        self.insert_ptr = self.insert_ptr+1 if self.insert_ptr+1 < len(self.buffer) else 0\n",
    "        self.upper_bound = max(self.insert_ptr, self.upper_bound)\n",
    "    \n",
    "    def sample(self, k: int = 1) -> List[ReplaySample]:\n",
    "        return random.choices(self.buffer[:self.upper_bound], k=k)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> ReplaySample:\n",
    "        if index < self.upper_bound:\n",
    "            return self.buffer[index]\n",
    "        raise IndexError()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPolicy(BasePlayer):\n",
    "    def __init__(\n",
    "            self, \n",
    "            layers=[32, 64], \n",
    "            device=\"cpu\", \n",
    "            training: bool = True,\n",
    "            replay_buffer_size: int = 1_000_000\n",
    "        ):\n",
    "        self.device = device\n",
    "        network = []\n",
    "        network.append(torch.nn.Linear(31, layers[0]))\n",
    "        network.append(torch.nn.ReLU())\n",
    "        for idx in range(1, len(layers)):\n",
    "            network.append(torch.nn.Linear(layers[idx-1], layers[idx]))\n",
    "            network.append(torch.nn.ReLU())\n",
    "        network.append(torch.nn.Linear(layers[-1], 1))\n",
    "        self.q_network = torch.nn.Sequential(*network).to(self.device)\n",
    "        self.t_network = torch.nn.Sequential(*network).to(self.device)\n",
    "        self.training = training\n",
    "        self.replay_buffer = ReplayBuffer(size=replay_buffer_size)\n",
    "        self.q_network.apply(self._init_weights)\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters())\n",
    "        self.prev_state_action = None\n",
    "        self.gamma = 1\n",
    "        self.lr = 0.001\n",
    "        self.grad_clip = 10\n",
    "        self.soft_epsilon = 0.05\n",
    "        self._sync_networks()\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if hasattr(m, \"weight\"):\n",
    "            torch.nn.init.xavier_uniform_(m.weight, gain=2 ** (1.0 / 2))\n",
    "        if hasattr(m, \"bias\"):\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _sync_networks(self) -> None:\n",
    "        logger.debug(\"syncing weights of t-network\")\n",
    "        self.t_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def _encode_state(self, game: Game) -> torch.Tensor:\n",
    "        dice = game.dice.copy()\n",
    "        if len(dice) < 4:\n",
    "            dice += [0] * (4 - len(dice))\n",
    "        return torch.concat([\n",
    "            torch.tensor(game.board),\n",
    "            torch.tensor(dice),\n",
    "            torch.tensor([game.head_moves])\n",
    "        ]).to(self.device).float()\n",
    "\n",
    "    def _encode_actions(self, actions: List[Tuple[int, int]]) -> torch.Tensor:\n",
    "        if not actions:\n",
    "            actions = [(-1,-1)]\n",
    "        return torch.tensor(actions).to(self.device).float()\n",
    "    \n",
    "    def _encode_state_actions(self, game: Game) -> torch.Tensor:\n",
    "        return self._concat_state_actions(\n",
    "            self._encode_state(game),\n",
    "            self._encode_actions(game.get_valid_moves())\n",
    "        )\n",
    "\n",
    "    def _concat_state_actions(self, state: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        actions = actions.view(-1, 2)\n",
    "        return torch.concat([\n",
    "                state.unsqueeze(dim=0).broadcast_to((actions.shape[0], -1)),\n",
    "                actions\n",
    "            ], dim=1)\n",
    "   \n",
    "    def _sample_batch(self, batch_size: int = 32):\n",
    "        rewards = []\n",
    "        curr_state_actions = []\n",
    "        next_state_actions = []\n",
    "        next_state_actions_idx = []\n",
    "        for sample_id, sample in enumerate(self.replay_buffer.sample(k=batch_size)):\n",
    "            rewards.append(sample.reward)\n",
    "            curr_state_actions.append(sample.state_action)\n",
    "            if sample.next_state_actions is not None:\n",
    "                next_state_actions.append(sample.next_state_actions)\n",
    "                next_state_actions_idx.extend([sample_id] * sample.next_state_actions.shape[0])\n",
    "\n",
    "        rewards = torch.tensor(rewards).float().to(self.device)\n",
    "        curr_state_actions = torch.vstack(curr_state_actions)\n",
    "        next_state_actions = torch.vstack(next_state_actions)\n",
    "        next_state_actions_idx = torch.tensor(next_state_actions_idx, dtype=torch.long).to(self.device)\n",
    "        assert next_state_actions.shape[0] == next_state_actions_idx.shape[0]\n",
    "        return curr_state_actions, rewards, next_state_actions, next_state_actions_idx\n",
    "    \n",
    "    def _calc_loss(self, q_scores, t_scores, sample_ids, rewards) -> torch.Tensor:\n",
    "        t_scores_max = rewards.scatter_reduce(dim=0, index=sample_ids, src=t_scores.squeeze(), reduce=\"max\", include_self=False)\n",
    "        # t_scores_max, t_score_idx = torch_scatter.scatter_max(t_scores.squeeze(), index=sample_ids, dim=0)\n",
    "        td_target = rewards + self.gamma * t_scores_max\n",
    "        td_error = td_target - q_scores.squeeze()\n",
    "        return td_error.pow(2).mean()\n",
    "    \n",
    "    def _train_step(self, batch_size: int = 32) -> None:\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            logger.warning(\"not enough samples in replay buffer, skipping train step\")\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "    \n",
    "        curr_sa, rewards, next_sa, sample_ids  = self._sample_batch(batch_size)\n",
    "\n",
    "        q_scores = self.q_network(curr_sa)\n",
    "        with torch.no_grad():\n",
    "            t_scores = self.t_network(next_sa)\n",
    "\n",
    "        loss = self._calc_loss(q_scores, t_scores, sample_ids, rewards)\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), self.grad_clip)\n",
    "            \n",
    "        for group in self.optimizer.param_groups:\n",
    "            group[\"lr\"] = self.lr\n",
    "        \n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), grad_norm.item()\n",
    "\n",
    "\n",
    "    def play_turn(self, game: Game):\n",
    "        valid_moves = list(game.get_valid_moves())\n",
    "        if not valid_moves:\n",
    "            game.skip()\n",
    "            return False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state_actions = self._encode_state_actions(game)\n",
    "\n",
    "            if self.training and self.prev_state_action is not None:\n",
    "                self.replay_buffer.add(ReplaySample(state_action=self.prev_state_action, reward=self._calc_reward(game), next_state_actions=state_actions))\n",
    "                self.prev_state_action = None\n",
    "        \n",
    "            if game._randfloat() < self.soft_epsilon:\n",
    "                action_idx = game._randint(0, len(valid_moves)-1)\n",
    "            else:\n",
    "                scores = self.q_network(state_actions).squeeze()\n",
    "                action_idx = scores.argmax(dim=-1).item()\n",
    "\n",
    "            self.prev_state_action = state_actions[action_idx]\n",
    "\n",
    "            game.turn(*valid_moves[action_idx])\n",
    "\n",
    "            if game.is_finished() and self.training and self.prev_state_action is not None:\n",
    "                self.replay_buffer.add(ReplaySample(state_action=self.prev_state_action, reward=self._calc_reward(game)))\n",
    "                self.prev_state_action = None\n",
    "\n",
    "            # self._train_step()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _calc_reward(self, game: Game) -> int:\n",
    "        reward = 0\n",
    "        if game.is_finished():\n",
    "            reward  = (2 if game.home[game._opponent] == 0 else 1)\n",
    "            reward *= (1 if game.pturn == 0 else -1)\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def practice(\n",
    "        policy: QPolicy, \n",
    "        games: int = 100,\n",
    "        train_every: int = 1, \n",
    "        sync_every: int = 100,\n",
    "        batch_size = 32,\n",
    "        show_progress : bool = False, \n",
    "        gamma: float = 0.99, \n",
    "        lr: float = 0.001,\n",
    "        grad_clip: float = 10,\n",
    "        soft_epsilon: float = 0\n",
    "    ):\n",
    "    policy.training = True\n",
    "    policy.gamma = gamma\n",
    "    policy.lr = lr\n",
    "    policy.grad_clip = grad_clip\n",
    "    policy.soft_epsilon = soft_epsilon\n",
    "\n",
    "    loss_vals = []\n",
    "    grad_vals = []\n",
    "    for game_id in (tqdm.trange(games, leave=False, desc=\"practicing\") if show_progress else range(games)):\n",
    "        simulate(AutoGame(), player1=policy, player2=RandomPlayer())\n",
    "        if game_id % train_every == 0:\n",
    "            loss, grad = policy._train_step(batch_size=batch_size)\n",
    "            loss_vals.append(loss)\n",
    "            grad_vals.append(grad)\n",
    "        if game_id % sync_every == 0:\n",
    "            policy._sync_networks()\n",
    "    return sum(loss_vals) / len(loss_vals), sum(grad_vals) / len(grad_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_name: str, policy: QPolicy, games: int = 100):\n",
    "    prev_training = policy.training\n",
    "    policy.training = False\n",
    "\n",
    "    results = []\n",
    "    sims = pd.DataFrame([simulate(AutoGame(player2=RandomPlayer()), player1=policy).__dict__ for _ in tqdm.trange(games, leave=False, desc=\"evaluating\")])\n",
    "    exp_info = {\"model\": model_name, \"p2\": \"random\", \"start\": \"random\"}\n",
    "    exp_info[\"games\"] = games\n",
    "    exp_info[\"wins\"] = sims[\"winner\"].sum()\n",
    "    wins_mu = exp_info[\"wins\"] / exp_info[\"games\"]\n",
    "    wins_sd = round(math.sqrt(exp_info[\"games\"] * wins_mu * (1 - wins_mu)), 2)\n",
    "    exp_info[\"win_rate_lo\"] = (exp_info[\"wins\"] - wins_sd*3) / exp_info[\"games\"]\n",
    "    exp_info[\"win_rate_mu\"] = wins_mu\n",
    "    exp_info[\"win_rate_hi\"] = (exp_info[\"wins\"] + wins_sd*3) / exp_info[\"games\"]\n",
    "    exp_info[\"avg_turns\"] = sims[\"turns\"].mean()\n",
    "    exp_info[\"avg_reward\"] = sims[\"reward\"].mean()\n",
    "    results.append(exp_info)\n",
    "\n",
    "    policy.training = prev_training\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_loop(\n",
    "    policy: QPolicy,\n",
    "    epochs: int = 1000,\n",
    "    practice_games: int = 1000,\n",
    "    batch_size: int = 32,\n",
    "    eval_games: int = 100,\n",
    "    sync_every: int = 50,\n",
    "    **kwargs\n",
    "):\n",
    "    epoch_pbar = tqdm.trange(1, epochs+1, desc=\"train/eval epochs\")\n",
    "    results = []\n",
    "    result = evaluate(f\"untrained\", policy, games=eval_games).loc[0].to_dict()\n",
    "    epoch_pbar.set_postfix({\"win_rate\": result[\"win_rate_mu\"], \"avg_reward\": result[\"avg_reward\"]})\n",
    "    logger.info(f\"untrained: win_rate={result['win_rate_mu']:.4%}, avg_reward={result['avg_reward']:.2f}\")\n",
    "    results.append(result)\n",
    "    for epoch_id in epoch_pbar:\n",
    "        avg_loss, avg_grad = practice(policy, games=practice_games, train_every=1, sync_every=sync_every, batch_size=batch_size, show_progress=True, **kwargs)\n",
    "        epoch_pbar.set_postfix({\"win_rate\": result[\"win_rate_mu\"], \"avg_reward\": result[\"avg_reward\"], \"avg_loss\": avg_loss, \"avg_grad\": avg_grad})\n",
    "        result = evaluate(f\"epoch-{epoch_id}\", policy, games=eval_games).loc[0].to_dict()\n",
    "        result[\"avg_loss\"] = avg_loss\n",
    "        result[\"avg_grad\"] = avg_grad\n",
    "        results.append(result)\n",
    "        epoch_pbar.set_postfix({\"win_rate\": result[\"win_rate_mu\"], \"avg_reward\": result[\"avg_reward\"], \"avg_loss\": avg_loss, \"avg_grad\": avg_grad})\n",
    "        logger.info(f\"epoch={epoch_id}: win_rate={result['win_rate_mu']:.4%}, avg_reward={result['avg_reward']:.2f}, {avg_loss=:.4f}, {avg_grad=:.4f}\")\n",
    "    results = pd.DataFrame(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_player = QPolicy(layers=[128, 512, 128], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_args = {\"epochs\": 100, \"practice_games\": 1000, \"batch_size\": 100, \"eval_games\": 100, \"sync_every\": 999}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3bd9c613e243c0a5a20595d69cf1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/eval epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428e08ade1104ebcb15a07dc2314c4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:20:07,111 - backgammon - INFO - untrained: win_rate=88.0000%, avg_reward=1.09\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d19a058a144f57af8fa74b6ded6559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743d303117394292bc16855418cd08af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:21:24,485 - backgammon - INFO - epoch=1: win_rate=33.0000%, avg_reward=-0.51, avg_loss=3.0531, avg_grad=28.2337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c229a387e42e4e1bad476529737028a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a9fb03f39646128fb059682109c891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:22:55,575 - backgammon - INFO - epoch=2: win_rate=56.0000%, avg_reward=0.16, avg_loss=1.2168, avg_grad=18.0708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff674f253f614612aabc867b2a165425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17a53b36af04531aff44909ce3823ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:24:17,242 - backgammon - INFO - epoch=3: win_rate=54.0000%, avg_reward=0.06, avg_loss=0.5852, avg_grad=11.5910\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd04aa957824b0c8bfa0f03db4f1aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4715902d6f647a6abb4bab2a03234ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:25:35,456 - backgammon - INFO - epoch=4: win_rate=37.0000%, avg_reward=-0.39, avg_loss=0.2793, avg_grad=7.0476\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd5b3518938499fb9fc4e7eeb44d991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d8ef31c9384907bf2fd8a2984b7a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:26:39,613 - backgammon - INFO - epoch=5: win_rate=34.0000%, avg_reward=-0.49, avg_loss=0.1833, avg_grad=5.4631\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574f4a38d2a7499494e7a515e8ea78fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89c8988c975499e8160d20c1b6d89bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:27:48,041 - backgammon - INFO - epoch=6: win_rate=49.0000%, avg_reward=-0.04, avg_loss=0.1327, avg_grad=4.5358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52fad1de2ae445c84e030cfdf511422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96123b2136424409b47b9234bd245a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:29:03,271 - backgammon - INFO - epoch=7: win_rate=49.0000%, avg_reward=-0.11, avg_loss=0.1084, avg_grad=4.0319\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4c3674a86f4a62ab5e117f0e1daf34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d491504c4124d749dbca187c055fbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:30:21,238 - backgammon - INFO - epoch=8: win_rate=49.0000%, avg_reward=0.04, avg_loss=0.0925, avg_grad=3.6981\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ef9053e14f492aab3f9fc0fb6e5052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecfc08e023c46aab32bc91111f7d4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:31:35,349 - backgammon - INFO - epoch=9: win_rate=43.0000%, avg_reward=-0.19, avg_loss=0.0774, avg_grad=3.2950\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcfed40686c451f9725bcd6d9013cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa780dc606e411f9bf784d7f995d546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:32:51,857 - backgammon - INFO - epoch=10: win_rate=40.0000%, avg_reward=-0.24, avg_loss=0.0672, avg_grad=3.0834\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea8bc37ddf745cdbe0d2f0b6740c03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee0e777aab34e0e95b9c1bc71eaf300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:34:04,581 - backgammon - INFO - epoch=11: win_rate=44.0000%, avg_reward=-0.24, avg_loss=0.0568, avg_grad=2.7851\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf16a3929154671be493cad2da38ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab7267cff4240bea9e798939792f8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:35:44,967 - backgammon - INFO - epoch=12: win_rate=51.0000%, avg_reward=-0.03, avg_loss=0.0498, avg_grad=2.5911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1faaa4672d4ad6ada8ae2e94fe9d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd6bccbc14647cabe54d6305025b2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:39:17,569 - backgammon - INFO - epoch=13: win_rate=49.0000%, avg_reward=-0.03, avg_loss=0.0429, avg_grad=2.3218\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919833fe7f53461e842adb01715b6b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440cf9ea3c0547bb9db604eb5a0488df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:40:29,866 - backgammon - INFO - epoch=14: win_rate=45.0000%, avg_reward=-0.12, avg_loss=0.0386, avg_grad=2.2617\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415da9bb5d8a47c5b49f8a22c9e0e6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7416d4924b3041008cf1b43980226ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:41:42,359 - backgammon - INFO - epoch=15: win_rate=54.0000%, avg_reward=0.09, avg_loss=0.0347, avg_grad=2.0094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cf639640d440718c73a6ee61e59e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bea3b727b2244f49d1d3242736d72b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:42:57,703 - backgammon - INFO - epoch=16: win_rate=46.0000%, avg_reward=-0.15, avg_loss=0.0326, avg_grad=2.0028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947b22595e2e41898ab04ce0871de37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac7846b11f74169b412827b5c956f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:44:11,099 - backgammon - INFO - epoch=17: win_rate=50.0000%, avg_reward=0.07, avg_loss=0.0289, avg_grad=1.8270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f357a81aef4950a679109f48f303f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31e834e68884009bdcd82580fc59b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 15:45:37,838 - backgammon - INFO - epoch=18: win_rate=37.0000%, avg_reward=-0.37, avg_loss=0.0265, avg_grad=1.8061\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae11d8a369984afc823b187e093a0ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "practicing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_eval_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_player\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.00001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoft_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_eval_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m, in \u001b[0;36mtrain_eval_loop\u001b[0;34m(policy, epochs, practice_games, batch_size, eval_games, sync_every, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_id \u001b[38;5;129;01min\u001b[39;00m epoch_pbar:\n\u001b[0;32m---> 17\u001b[0m     avg_loss, avg_grad \u001b[38;5;241m=\u001b[39m \u001b[43mpractice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpractice_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msync_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     epoch_pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin_rate_mu\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_grad})\n\u001b[1;32m     19\u001b[0m     result \u001b[38;5;241m=\u001b[39m evaluate(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, policy, games\u001b[38;5;241m=\u001b[39meval_games)\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m, in \u001b[0;36mpractice\u001b[0;34m(policy, games, train_every, sync_every, batch_size, show_progress, gamma, lr, grad_clip, soft_epsilon)\u001b[0m\n\u001b[1;32m     20\u001b[0m grad_vals \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m game_id \u001b[38;5;129;01min\u001b[39;00m (tqdm\u001b[38;5;241m.\u001b[39mtrange(games, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpracticing\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m show_progress \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(games)):\n\u001b[0;32m---> 22\u001b[0m     \u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAutoGame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRandomPlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m game_id \u001b[38;5;241m%\u001b[39m train_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m         loss, grad \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39m_train_step(batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/Documents/code/rl-excercises/backgammon/game.py:467\u001b[0m, in \u001b[0;36msimulate\u001b[0;34m(game, player1, player2)\u001b[0m\n\u001b[1;32m    465\u001b[0m     player \u001b[38;5;241m=\u001b[39m player1 \u001b[38;5;28;01mif\u001b[39;00m (game\u001b[38;5;241m.\u001b[39mpturn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m player2\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m player:\n\u001b[0;32m--> 467\u001b[0m         \u001b[43mplayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summarize(game)\n",
      "File \u001b[0;32m~/Documents/code/rl-excercises/backgammon/game.py:374\u001b[0m, in \u001b[0;36mRandomPlayer.play_turn\u001b[0;34m(self, game)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(actions) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    373\u001b[0m     pos, steps \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39m_randchoice(actions)\n\u001b[0;32m--> 374\u001b[0m     \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mturn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/code/rl-excercises/backgammon/game.py:422\u001b[0m, in \u001b[0;36mAutoGame.turn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mturn\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoGame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mturn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_automate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/rl-excercises/backgammon/game.py:406\u001b[0m, in \u001b[0;36mAutoGame._automate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroll()\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m==\u001b[39m Game\u001b[38;5;241m.\u001b[39mStep\u001b[38;5;241m.\u001b[39mTURN:\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_valid_moves\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip()\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/code/rl-excercises/backgammon/game.py:180\u001b[0m, in \u001b[0;36mGame.has_valid_moves\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_valid_moves\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_valid_moves\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/code/rl-excercises/backgammon/game.py:176\u001b[0m, in \u001b[0;36mGame.get_valid_moves\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_valid_moves\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_moves \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_moves \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enum_valid_moves\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_moves\n",
      "File \u001b[0;32m~/Documents/code/rl-excercises/backgammon/game.py:169\u001b[0m, in \u001b[0;36mGame._enum_valid_moves\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_enum_valid_moves\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m25\u001b[39m):\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_checkers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cur_player\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    170\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m steps \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m    171\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_valid_move(pos, steps):\n",
      "File \u001b[0;32m~/Documents/code/rl-excercises/backgammon/game.py:100\u001b[0m, in \u001b[0;36mGame._has_checkers\u001b[0;34m(self, pos, player)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_has_checkers\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos: \u001b[38;5;28mint\u001b[39m, player: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_checkers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/code/rl-excercises/backgammon/game.py:102\u001b[0m, in \u001b[0;36mGame._get_checkers\u001b[0;34m(self, pos, player)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_has_checkers\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos: \u001b[38;5;28mint\u001b[39m, player: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_checkers(pos, player) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_checkers\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos: \u001b[38;5;28mint\u001b[39m, player: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    103\u001b[0m     player \u001b[38;5;241m=\u001b[39m player \u001b[38;5;28;01mif\u001b[39;00m player \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpturn\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_user_sign(player) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard[pos\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_eval_loop(nn_player, lr=0.00001, gamma=0.99, grad_clip=3, soft_epsilon=0.1, **train_eval_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with scatter & reduce:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executed on MPS, scatter_reduce implementation fails with the following error:\n",
    "\n",
    "NotImplementedError: The operator 'aten::scatter_reduce.two_out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_score = nn_player.q_network(curr_state_actions)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     t_score = nn_player.t_network(next_state_actions).squeeze()\n",
    "#     t_score_max = torch.zeros_like(rewards).scatter_reduce(0, index=next_state_actions_idx, src=t_score, reduce=\"max\", include_self=False)\n",
    "    \n",
    "# t_score_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this implementation fails on MPS with another error:\n",
    "\n",
    "RuntimeError: src.device().is_cpu() INTERNAL ASSERT FAILED at \"csrc/cpu/scatter_cpu.cpp\":11, please report a bug to PyTorch. src must be CPU tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_score = nn_player.q_network(curr_state_actions)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     t_score = nn_player.t_network(next_state_actions).squeeze()\n",
    "#     t_score_max, t_score_idx = torch_scatter.scatter_max(t_score, index=next_state_actions_idx, dim=0)\n",
    "    \n",
    "# t_score_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_score, max_score_idx = torch_scatter.scatter_max(score, index=next_state_actions_idx, dim=0)\n",
    "# max_score, max_score_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.zeros_like(rewards).scatter_reduce(0, index=next_state_actions_idx, src=score.squeeze(), reduce=\"max\", include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_score, max_score_idx = torch_scatter.scatter_max(score, index=next_state_actions_idx, dim=0)\n",
    "# max_score, max_score_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.arange(24).view(-1,6).long()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = data.float().sum(dim=1)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_score, max_score_idx = torch_scatter.scatter_max(score, index=torch.tensor([0,0,0,1]), dim=0)\n",
    "# max_score, max_score_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[max_score_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XCS234",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
